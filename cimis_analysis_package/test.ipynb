{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb29d06-8c87-429b-8536-9441c07816fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root (derived from CWD): /home/salba/SpatialCIMIS\n",
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Base Project Path (assuming this file is in cimis_analysis_package) ---\n",
    "# This helps in creating absolute paths if needed, especially for data/output\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "print(f\"Project Root (derived from CWD): {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Input Data Paths ---\n",
    "STATION_API_URL = 'http://et.water.ca.gov/api/station'\n",
    "# Default path for CIMIS_Stations.csv, can be overridden by main script if downloaded elsewhere\n",
    "DEFAULT_STATION_CSV_PATH = os.path.join(PROJECT_ROOT, \"data\", \"CIMIS_Stations.csv\")\n",
    "CIMIS_DATA_PATH = \"/group/moniergrp/SpatialCIMIS/CIMIS/\"  # MODIFY if your path is different\n",
    "NETCDF_SPATIAL_CIMIS_PATH = '/group/moniergrp/SpatialCIMIS/netcdf/'  # MODIFY if your path is different\n",
    "NETCDF_GRIDMET_PATH = \"/group/moniergrp/gridMET\"  # MODIFY if your path is different\n",
    "DEFAULT_SHAPEFILE_PATH = os.path.join(PROJECT_ROOT, \"data\", \"CA_State.shp\") # Ensure this shapefile is available\n",
    "\n",
    "# --- Output Base Directories ---\n",
    "# The main script will create subdirectories for each variable within these.\n",
    "OUTPUT_BASE_DIR = os.path.join(PROJECT_ROOT, \"output\")\n",
    "OUTPUT_NETCDF_PATH_BASE = os.path.join(OUTPUT_BASE_DIR, \"output_netcdf\")\n",
    "OUTPUT_CSV_PATH_BASE = os.path.join(OUTPUT_BASE_DIR, \"output_csv\")\n",
    "OUTPUT_PLOT_PATH_BASE = os.path.join(OUTPUT_BASE_DIR, \"output_plots\")\n",
    "\n",
    "\n",
    "# --- Date Filters ---\n",
    "START_DATE_FILTER = '2004-01-01'\n",
    "END_DATE_FILTER = '2024-01-01'\n",
    "CONNECT_DATE_FILTER = '2004-01-01'\n",
    "DISCONNECT_DATE_FILTER = '2024-01-01'\n",
    "\n",
    "# --- Variable Configuration ---\n",
    "TARGET_VARIABLES_CONFIG = {\n",
    "    \"Rad\": {\n",
    "        \"cimis_csv_column_name\": \"Sol Rad (W/sq.m)\",\n",
    "        \"cimis_csv_output_name\": \"Rad\",\n",
    "        \"spatial_cimis_netcdf_var\": \"Rs\",\n",
    "        \"gridmet_netcdf_var\": \"srad\", # This is the prefix for gridMET files, actual var name inside might differ\n",
    "        \"gridmet_internal_var_name_heuristic\": \"radiation\", # Heuristic to find actual var name in gridMET file\n",
    "        \"unit\": \"W/m²\",\n",
    "        \"spatial_cimis_scale_factor\": 11.57,\n",
    "        \"gridmet_scale_factor\": 0.1\n",
    "    },\n",
    "    \"ETo\": {\n",
    "        \"cimis_csv_column_name\": \"ETo (mm)\",\n",
    "        \"cimis_csv_output_name\": \"ETo\",\n",
    "        \"spatial_cimis_netcdf_var\": \"ETo\",\n",
    "        \"gridmet_netcdf_var\": \"eto\", # Prefix for gridMET files\n",
    "        \"gridmet_internal_var_name_heuristic\": \"evapotranspiration\", # Heuristic\n",
    "        \"unit\": \"mm/day\",\n",
    "        \"spatial_cimis_scale_factor\": 1.0,\n",
    "        \"gridmet_scale_factor\": 1.0\n",
    "    },\n",
    "    # Example for Temperature Minimum\n",
    "    # \"Tmin\": {\n",
    "    #     \"cimis_csv_column_name\": \"Min Temp (°C)\", # Verify actual column name in CIMIS CSVs\n",
    "    #     \"cimis_csv_output_name\": \"Tmin\",\n",
    "    #     \"spatial_cimis_netcdf_var\": \"Tmin\", # Or appropriate var if from a different spatial product\n",
    "    #     \"gridmet_netcdf_var\": \"tmmn\",       # Prefix for gridMET files\n",
    "    #     \"gridmet_internal_var_name_heuristic\": \"minimum_temperature\", # Heuristic\n",
    "    #     \"unit\": \"°C\",\n",
    "    #     \"spatial_cimis_scale_factor\": 1.0,\n",
    "    #     \"gridmet_scale_factor\": 1.0 # GridMET temperatures are usually in Kelvin, may need conversion\n",
    "    # },\n",
    "}\n",
    "\n",
    "# --- Variables to Process in the main script ---\n",
    "VARIABLES_TO_PROCESS = [\"Rad\"]  # e.g., [\"Rad\", \"ETo\"]\n",
    "\n",
    "# --- Plotting Constants ---\n",
    "DEFAULT_STATION_CANDIDATES_FOR_TIMESERIES = ['6', '160']\n",
    "DEFAULT_PLOT_COLORS = {'station': '#ababab', 'spatial': 'blue', 'gridmet': 'red'}\n",
    "\n",
    "# --- Helper to create directories ---\n",
    "def ensure_dir_exists(path):\n",
    "    \"\"\"Ensures a directory exists, creating it if necessary.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Ensure base output directories exist when this config is loaded\n",
    "ensure_dir_exists(OUTPUT_NETCDF_PATH_BASE)\n",
    "ensure_dir_exists(OUTPUT_CSV_PATH_BASE)\n",
    "ensure_dir_exists(OUTPUT_PLOT_PATH_BASE)\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a32fc12-3bbe-4545-8ece-943a7f5b6fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data_fetching module...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_and_prepare_station_list() missing 1 required positional argument: 'DEFAULT_STATION_CSV_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting data_fetching module...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# 1. Try to load, which should trigger API fetch if file doesn't exist\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m stations \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_station_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoaded stations sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_and_prepare_station_list() missing 1 required positional argument: 'DEFAULT_STATION_CSV_PATH'"
     ]
    }
   ],
   "source": [
    "# cimis_analysis_package/data_fetching.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Import configuration constants\n",
    "# from . import config\n",
    "\n",
    "def fetch_and_save_station_list_from_api(STATION_API_URL, \n",
    "                                         DEFAULT_STATION_CSV_PATH):\n",
    "    \"\"\"\n",
    "    Fetches station data from the CIMIS API and saves it to a CSV file.\n",
    "    Args:\n",
    "        api_url (str): The URL for the CIMIS station API.\n",
    "        output_csv_path (str): Path to save the fetched station data.\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame of station data if successful, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching station data from API: {api_url}\")\n",
    "        with urllib.request.urlopen(api_url) as response:\n",
    "            content_bytes = response.read()\n",
    "            content_str = content_bytes.decode('utf-8') # Ensure proper decoding\n",
    "            content = json.loads(content_str)\n",
    "            \n",
    "        stations_data = content.get(\"Stations\", None) # Use .get for safer access\n",
    "\n",
    "        if stations_data:\n",
    "            df = pd.DataFrame.from_dict(stations_data)\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "            df.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Successfully fetched and saved station data to {output_csv_path}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No 'Stations' key found in API response or data is empty.\")\n",
    "            return None\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"URL Error fetching station data: {e}. Check network or API URL.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error fetching station data: {e}. API response might not be valid JSON.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred fetching or saving station data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_prepare_station_list(DEFAULT_STATION_CSV_PATH):\n",
    "    \"\"\"\n",
    "    Loads the station list from a CSV file and prepares it as a GeoDataFrame.\n",
    "    Args:\n",
    "        csv_path (str): Path to the station list CSV file.\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame or None: GeoDataFrame of prepared station data, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Station CSV file not found at {csv_path}. Attempting to fetch from API.\")\n",
    "            fetch_and_save_station_list_from_api(output_csv_path=csv_path)\n",
    "            if not os.path.exists(csv_path): # Check again if fetch failed\n",
    "                 print(f\"Failed to fetch station data. Cannot load station list from {csv_path}.\")\n",
    "                 return None\n",
    "\n",
    "        print(f\"Loading station list from: {csv_path}\")\n",
    "        station_list_df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Data cleaning and preparation\n",
    "        station_list_df['HmsLatitude'] = station_list_df['HmsLatitude'].astype(str).str.split('/').str[-1].str.strip()\n",
    "        station_list_df['HmsLongitude'] = station_list_df['HmsLongitude'].astype(str).str.split('/').str[-1].str.strip()\n",
    "        station_list_df['Name'] = station_list_df['Name'].astype(str).str.replace(\" \", \"\").str.replace(\"/\", \"\").str.replace(\".\", \"\", regex=False).str.replace(\"-\", \"\", regex=False)\n",
    "        \n",
    "        station_list_df['DisconnectDate'] = pd.to_datetime(station_list_df['DisconnectDate'], errors='coerce')\n",
    "        station_list_df['ConnectDate'] = pd.to_datetime(station_list_df['ConnectDate'], errors='coerce')\n",
    "        \n",
    "        station_list_df.rename(columns={'HmsLatitude': 'Latitude', 'HmsLongitude': 'Longitude'}, inplace=True)\n",
    "        \n",
    "        station_list_df['Latitude'] = pd.to_numeric(station_list_df['Latitude'], errors='coerce')\n",
    "        station_list_df['Longitude'] = pd.to_numeric(station_list_df['Longitude'], errors='coerce')\n",
    "        \n",
    "        # Drop rows where essential geo-coordinates are missing after coercion\n",
    "        station_list_df.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "        if station_list_df.empty:\n",
    "            print(\"No valid station data after cleaning coordinates.\")\n",
    "            return None\n",
    "\n",
    "        stations_gdf = gpd.GeoDataFrame(\n",
    "            station_list_df,\n",
    "            geometry=gpd.points_from_xy(station_list_df.Longitude, station_list_df.Latitude),\n",
    "            crs=\"EPSG:4326\"  # Standard CRS for lat/lon\n",
    "        )\n",
    "        print(f\"Station list loaded and prepared into GeoDataFrame. {len(stations_gdf)} stations.\")\n",
    "        return stations_gdf\n",
    "    except FileNotFoundError: # Should be caught by os.path.exists now, but good fallback\n",
    "        print(f\"Error: Station CSV file not found at {csv_path} and could not be fetched.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preparing station list: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (and a simple test)\n",
    "print(\"Testing data_fetching module...\")\n",
    "# 1. Try to load, which should trigger API fetch if file doesn't exist\n",
    "stations = load_and_prepare_station_list()\n",
    "if stations is not None:\n",
    "    print(\"\\nLoaded stations sample:\")\n",
    "    print(stations.head())\n",
    "else:\n",
    "    print(\"\\nFailed to load stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b43f0-bd82-4f07-97b8-cbdf3f0be637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
